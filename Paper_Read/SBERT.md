# 论文阅读

## 2025.7.7: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

### 1. 这篇文章解决了什么问题：

BERT在解决两语句相似度时，由基础BERT模型得出的embedding的相似度比较非常差，因此传统的利用BERT进行相似度查找的时候，常将两个语句连接后得到相应的embedding。因此对于n个句子需要有n(n-1)/2次求出embedding值。时间复杂度为O(n^2)。这篇文章通过利用三个function来微调BERT，得到的SBERT能够更好地求出单个语句的embedding，从而使得只需要对原始文本求出embedding值，相互比较则可找到相似度最高的语句。时间复杂度为O(n)，从而极大降低了算力成本。

### 2. 这个问题为什么重要，重要在什么地方？

语句相似度问题的不断解决，能够使内容的查找更加精确。从而增强大模型的能力。O(n^2)使得模型参数扩大会导致训练成本的急剧上升。算力对于大模型极其关键。SBERT进一步减少了模型推理成本，增强了模型的性能。

### 3. 原来大家怎么解决这个问题的？

过去更多是通过训练新的模型来取得更好的效果，而很少直接在当时具有强大功能的BERT的基础上进行微调。

### 4. 这篇文章和原来方法的差异是什么？创新点是什么？

这种方法是充分利用了BERT原有基础的权重。在原有的网络基础上进行加入特殊的，专注于单个语句embedding的层，来微调模型，使得模型具备更好的关于语句embedding的效果。

### 5. 和我们当前想做的关联是什么？

对于优化RAG来获得更好的效果，主要从两个方面入手，一个是加入优化分块，另外一个引入更高质量的embedding模型。SBERT主要做的工作就是在2019年通过微调BERT来使得embedding的效果更好。虽然现在有更强大的模型例如**text-embedding-v4**，但是text-embedding或许就是通过某个基座模型的微调得到的模型。

### 6. 我们想解决的问题和文章解决的问题的共同点和差异点是什么？能不能用当前文章的思路解决？预期会遇到什么问题？

**共同点**在于，都期望通过某种操作来提高对单个语句或文本块的embedding的精确水平，并且和现在优化模型的思路总体一致，即通过微调而不是再训练。**不同点**在于，SBERT在2019年的时候模型性能属于顶尖梯队，并且训练成本相较于2025年今天的模型非常小。当前要提高embedding的精确度水平，则必然要在诸如Gemini-2.5pro，GPT-o3，DeepSeek-R1等模型的基础上进行微调，其微调的成本很高

如果用文章的思路来解决当前的问题，则是通过设计更好的function，在诸如DeepSeek-R1的基础上进行微调，得到专一于文本块embedding精确度的提升。这是一个非常不错的思路。预期之中，可能会遇到根本不会提升，反而会下降的问题。因为现在DeepSeek-R1这种模型的原有的架构，往往已经非常完备。在这个基础上进行微调往往不是最优解，反而可能会降低模型的性能，并且600B+的数据微调起来仍然成本很高。