# 论文阅读

## 2025.7.7 Can Cross Encoders Produce Useful Sentence Embeddings?

### 1. 这篇论文解决了什么问题？

相较于CE，DE的精准度较低，并且由于其对数据的高质量要求，使得DE的训练十分困难。这篇论文通过将CE的embedding layer和layer 0的权重直接移植到DE，从而实现一种“知识注入”。并且发现以12层的DE(baseline)作为基准，两层随机权重开始训练的速度和精确度均远低于知识注入的DE-2-CE。从而解决了DE的训练困难的问题。

### 2. 这个问题为什么重要，重要在什么地方？

随着AI的发展，高质量的数据集越来越少，模型的训练需求却仍然增加。DE直接由sentence直接生成embedding，其必然要求更高质量的语料。因而数据成本很高。随着算力越来越高，算法越来越完善，数据将成为模型的最大制约因素。因而这种知识注入的方法能够一定程度上缓解数据压力。

### 3. 原来大家是如何解决这个问题的？

原来大家通过直接训练DE，或者由相应的CE微调而来。

### 4. 这篇文章和原来的方法的差异是什么？创新点是什么？

差异和创新点就是通过知识注入的方式来更快更好训练出来DE。

### 5. 和我们当前想做的关联是什么？

关于RAG的优化，其中必然要设计rerank。而rerank的关键就是DE和CE的性能。这篇文章进一步提供了一种增加CE模型的性能和降低训练成本的方法。

### 6. 我们想解决的问题和文章想解决的问题的共同点和差异点是什么？能不能用当前文章的思路解决？预期会遇到什么问题？

共同点是，文章解决的这个问题可以在一定程度上通过提升CE的性能来提升RAG的效果。可以尝试用文章的思路，来在一个性能更强大的Cross encoder上来进行知识注入到新模型，将新模型进行训练可能会得到更好的DE，从而提升整体的RAG效果。