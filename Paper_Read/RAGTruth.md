# RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models

## 笔记：

### 1. 幻觉检测的三种方法：

- 1. 利用外部模型对生成回答进行打分，从而检测幻觉。
- 2. 通过对生成回答进行外部知识的查找，从而检测是否是幻觉。
- 3. 基于LLM的自我检测：
    1. **Consistency-based:**: 通过多次生成、相互比对、打分判断等来检测。核心原理是在多次检测的情况下，掌握的知识每次都是一致的，而幻觉将每次都不同
    2. **Uncertainly-based:**检查生成token的概率，当概率很低或entropy很高的时候，将token或者前在的sentence标记为前在的幻觉。

## 总结：

### 1. 这篇论文解决了什么问题？

即使是基于给定信息库的RAG，也容易产生很多幻觉。这篇论文设计了一个高质量的关于幻觉的数据集，从而作为一个benchmark来评测RAG的幻觉率，或者作为训练内容来微调模型，使得微调模型具备更好的RAG效果。

### 2. 这个问题为什么重要，重要在什么地方？

随着大模型的发展，实时信息和基于给定信息的问答越来越重要。而基于给定信息的问答功能是除去parser、embedding之外的RAG的关键所在。即使是如GPT-4-turbo，在给定信息的问答方面仍然会产生很多幻觉。而过去对给定信息基础上问答的幻觉的评测数据集很少。从而很难对该问答效果进行评测。

### 3. 原来大家都是怎么解决这个问题的？

过去大家直接依赖于更强大的大模型或者一些通用的数据集进行测评，没有出现一个专业于RAG的评测数据集。

### 4. 这篇文章和原来文章的差异是什么？创新点是什么？

这篇文章首次创建了RAG专用的大模型幻觉评估数据集，其次过去的幻觉评估大都停留在sentence的句意上的幻觉评估，这篇文章首次从word层面上进行幻觉评估。

### 5. 和我们当前想做的关联是什么？

这篇文章为我们提供了一个对于RAG最后一部分generation的benchmark，提供了生成质量的量化评估效果。同时也揭示了小模型微调的巨大潜力。

### 6. 我们想解决的问题和文章想解决的问题的共同点和差异点是什么？能不能用当前文章的思路解决？预期会遇到什么问题？

共同点是，我们的目标和这篇文章的目标一致，都是为了提升RAG系统的可靠性和准确性。这篇文章的思路本质上是“通过高质量数据进行评测和优化”。这个思路是完全可以为我们所用的。但是应用预期会遇到工程问题。此外，RAGTruth的数据主要来源于通用问答、商业评论和新闻，如果我们要专注于考研知识题目问答和organoidchat，其效果不一定可以完美迁移。但是RAGTruth的思路是可以借鉴的。

