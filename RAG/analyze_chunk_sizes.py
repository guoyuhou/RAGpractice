import os
import sys
import json
import ast
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from openai import OpenAI

# --- [å…³é”®] ç¡®ä¿èƒ½å¯¼å…¥æ‚¨çš„RAGå‡½æ•°å’ŒFRAMES_benchä¸­çš„å‡½æ•° ---
# å‡è®¾æ­¤è„šæœ¬ä¸RAG_Benchæ–‡ä»¶å¤¹å’Œsimple_ragæ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸‹
# (å¦‚æœç›®å½•ç»“æ„ä¸åŒï¼Œè¯·ç›¸åº”è°ƒæ•´)
from simple_rag import main as my_rag_functions
from RAG_Bench.FRAMES_bench import get_content_from_urls # å¤ç”¨ç½‘é¡µæŠ“å–å‡½æ•°

# --- é…ç½®åŒº ---
DASHSCOPE_API_KEY = "sk-fb8191fb105b439d9ffd2880a9d9be7c"
# åˆå§‹åŒ–å®¢æˆ·ç«¯
dashscope_client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

def analyze_chunking(test_samples):
    """
    åˆ†æä¸åŒåˆ†å—æ–¹æ³•å¯¹æ–‡æœ¬çš„å¤„ç†ç»“æœã€‚
    """
    print("\n" + "="*50)
    print("ğŸš€ å¼€å§‹åˆ†æä¸åŒåˆ†å—(Chunking)æ–¹æ³•çš„å°ºå¯¸...")
    print("="*50)

    chunking_methods = {
        'fix_size': my_rag_functions.chunk_by_fix_size,
        'recursive': my_rag_functions.chunk_recursively,
        'semantic': lambda text: my_rag_functions.chunk_semantic(text, client=dashscope_client)
    }
    
    # å­˜å‚¨æ‰€æœ‰åˆ†æç»“æœ
    results = {name: [] for name in chunking_methods.keys()}

    for sample in tqdm(test_samples, desc="Analyzing documents"):
        knowledge_urls = ast.literal_eval(sample['wiki_links'])
        full_context_text = get_content_from_urls(knowledge_urls)
        
        if not full_context_text:
            continue

        for name, chunk_func in chunking_methods.items():
            chunks = chunk_func(full_context_text)
            chunk_lengths = [len(c) for c in chunks]
            
            if not chunk_lengths: # å¦‚æœæ²¡æœ‰äº§ç”Ÿä»»ä½•chunk
                continue

            stats = {
                'num_chunks': len(chunks),
                'avg_chunk_size': np.mean(chunk_lengths),
                'std_dev_size': np.std(chunk_lengths), # æ ‡å‡†å·®ï¼Œçœ‹å°ºå¯¸æ˜¯å¦ç¨³å®š
                'min_size': np.min(chunk_lengths),
                'max_size': np.max(chunk_lengths)
            }
            results[name].append(stats)

    # --- æ‰“å°æœ€ç»ˆçš„æ±‡æ€»æŠ¥å‘Š ---
    print("\n" + "="*60)
    print("ğŸ“Š åˆ†å—ç­–ç•¥åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    for name, stats_list in results.items():
        if not stats_list:
            print(f"\n--- æ–¹æ³•: {name} ---")
            print("æœªèƒ½ç”Ÿæˆä»»ä½•åˆ†å—ã€‚")
            continue
            
        avg_num_chunks = np.mean([s['num_chunks'] for s in stats_list])
        avg_chunk_size = np.mean([s['avg_chunk_size'] for s in stats_list])
        avg_std_dev = np.mean([s['std_dev_size'] for s in stats_list])
        
        print(f"\n--- æ–¹æ³• (Method): {name} ---")
        print(f"    å¹³å‡ç”Ÿæˆåˆ†å—æ•°é‡ (Avg. Chunks): {avg_num_chunks:.2f}")
        print(f"    å¹³å‡åˆ†å—é•¿åº¦ (Avg. Chunk Size): {avg_chunk_size:.2f} å­—ç¬¦")
        print(f"    å¹³å‡å°ºå¯¸æ ‡å‡†å·® (Avg. Std Dev): {avg_std_dev:.2f}")
    print("="*60)


if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½FRAMESæ•°æ®é›†...")
    frames_dataset = load_dataset("google/frames-benchmark")
    test_set = frames_dataset['test']
    
    # éšæœºæŒ‘é€‰3ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿåˆ†æ
    shuffled_test_set = test_set.shuffle(seed=42)
    test_samples = list(shuffled_test_set.select(range(3)))
    print(f"å·²éšæœºæŒ‘é€‰ {len(test_samples)} ä¸ªæ ·æœ¬è¿›è¡Œå°ºå¯¸åˆ†æã€‚")

    # 2. è¿è¡Œåˆ†æ
    analyze_chunking(test_samples)