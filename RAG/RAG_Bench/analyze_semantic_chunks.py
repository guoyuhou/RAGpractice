from datasets import load_dataset
from bs4 import BeautifulSoup
import requests
from simple_rag import main as my_rag_functions
from openai import OpenAI
from mistralai import Mistral
import os
from tqdm import tqdm
import ast
import json
import nltk
import time
from simple_rag import main as my_rag_function
import numpy as np
import pandas as pd

DASHSCOPE_API_KEY = "sk-fb8191fb105b439d9ffd2880a9d9be7c"
dashscope_client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")


def get_content_from_urls(urls, max_retries=3, initial_delay=1):
    """
    æ¥æ”¶ä¸€ä¸ªURLåˆ—è¡¨,æŠ“å–æ¯ä¸ªé¡µé¢çš„æ–‡æœ¬å†…å®¹å¹¶åˆå¹¶ã€‚
    """
    full_text_from_web = ''
    for url in urls:
        if url is None:
            continue
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=15)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')
                paragraphs = soup.find_all('p')
                for p in paragraphs:
                    full_text_from_web += p.get_text() + '\n'

                break
            
            except requests.exceptions.RequestException as e:
                print(f'æŠ“å–URLå¤±è´¥:{url},å°è¯•æ¬¡æ•°{attempt + 1}/{max_retries}...')
                print(f'é”™è¯¯{e}')

                if attempt < max_retries - 1:
                    delay = initial_delay * (2 ** attempt)
                    print(f'å°†åœ¨{delay}ç§’åå°è¯•')
                    time.sleep(delay)
                else:
                    print(f'å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,æ”¾å¼ƒæŠ“å–URL:{url}')
    
    return full_text_from_web

def analyze_and_collect_chunks(test_samples):
    """
    è¿è¡Œsemantic  chunkingå¹¶æ”¶é›†æ‰€æœ‰chunkæ•°æ®
    """
    print("\n" + "="*50)
    print("ğŸš€ å¼€å§‹è¿è¡ŒSemantic Chunkerå¹¶æ”¶é›†æ•°æ®...")
    print("="*50)

    all_chunks_data = []

    for i, sample in enumerate(tqdm(test_samples, desc='Processing documents')):
        try:
            knowledge_urls = ast.literal_eval(sample['wiki_links'])
        except (ValueError, SyntaxError):
            continue

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text:
            continue
        
        semantic_chunks = my_rag_function.chunk_semantic(full_context_text, client=dashscope_client)

        for chunk in semantic_chunks:
            all_chunks_data.append({
                'source_id': i,
                'chunk_content': chunk,
                'chunk_size': len(chunk)
            })

    df = pd.DataFrame(all_chunks_data)
    output_path = 'semantic_chunks_analysis.csv'
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nâœ… æ•°æ®æ”¶é›†å®Œæˆï¼å…±ç”Ÿæˆ {len(df)} ä¸ªè¯­ä¹‰å—ã€‚")
    print(f"ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    return df

if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½FRAMESæ•°æ®é›†...")
    frames_dataset = load_dataset('google/frames-benchmark')
    test_set = frames_dataset['test']
    
    # éšæœºæŒ‘é€‰æ ·æœ¬è¿›è¡Œåˆ†æ (å¯ä»¥å¢åŠ æ•°é‡ä»¥è·å¾—æ›´å¯é çš„ç»Ÿè®¡ç»“æœ)
    shuffled_test_set = test_set.shuffle(seed=42)
    test_samples = list(shuffled_test_set.select(range(50))) # ä½¿ç”¨å…¨éƒ¨50ä¸ªæ ·æœ¬
    print(f"å·²éšæœºæŒ‘é€‰ {len(test_samples)} ä¸ªæ ·æœ¬è¿›è¡Œåˆ†å—åˆ†æã€‚")

    # 2. è¿è¡Œåˆ†æå’Œæ”¶é›†
    analyze_and_collect_chunks(test_samples)