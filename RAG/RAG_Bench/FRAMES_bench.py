from datasets import load_dataset
from bs4 import BeautifulSoup
import requests
from simple_rag import main as my_rag_functions
from openai import OpenAI
from mistralai import Mistral
import os
from tqdm import tqdm
import ast
import json
import nltk
import time
nltk.download('punkt')


dashscope_key = 'sk-fb8191fb105b439d9ffd2880a9d9be7c'
mistral_key = 'NMRPcolmwB7PEegVbzi6TId4IUVCfCFB'
deepseek_key = "sk-09IfmX7mg7MJQg0U0OOM8AVXBrGC7Um887xvdb4H3Tbn16sQ"
Kimi_key = 'sk-vqaNEYl0Z3vf8GITnaebDySglXyuReqPLzGDlISr9qyKYEYo'


dashscope_client = OpenAI(api_key=dashscope_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
mistral_client = Mistral(api_key=mistral_key)
deepseek_client = OpenAI(api_key=deepseek_key, base_url="https://api.lkeap.cloud.tencent.com/v1")
kimik2_client = OpenAI(api_key=Kimi_key, base_url="https://api.moonshot.cn/v1")




def get_content_from_urls(urls, max_retries=3, initial_delay=1):
    """
    æ¥æ”¶ä¸€ä¸ªURLåˆ—è¡¨,æŠ“å–æ¯ä¸ªé¡µé¢çš„æ–‡æœ¬å†…å®¹å¹¶åˆå¹¶ã€‚
    """
    full_text_from_web = ''
    for url in urls:
        if url is None:
            continue
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=15)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')
                paragraphs = soup.find_all('p')
                for p in paragraphs:
                    full_text_from_web += p.get_text() + '\n'

                break
            
            except requests.exceptions.RequestException as e:
                print(f'æŠ“å–URLå¤±è´¥:{url},å°è¯•æ¬¡æ•°{attempt + 1}/{max_retries}...')
                print(f'é”™è¯¯{e}')

                if attempt < max_retries - 1:
                    delay = initial_delay * (2 ** attempt)
                    print(f'å°†åœ¨{delay}ç§’åå°è¯•')
                    time.sleep(delay)
                else:
                    print(f'å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,æ”¾å¼ƒæŠ“å–URL:{url}')
    
    return full_text_from_web

def save_results(results_data, filename_prefix):
    json_filename = f'{filename_prefix}_results.json'

    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, indent=4, ensure_ascii=False)
    print(f'JSONç»“æœå·²ç»ä¿å­˜è‡³: {json_filename}')

def run_chunking_experiment(test_samples):
    print('\n' + '='*50)
    print('å¼€å§‹å®éªŒä¸€: æµ‹è¯•ä¸åŒChunkingæ–¹æ³•(åŠ¨æ€Top-K)')
    print('\n' + '='*50)

    
    chunking_configs = {
        'fix_size': {
            'function': my_rag_functions.chunk_by_fix_size,
            'k': 5
        },
        'recursive':{
            'function': my_rag_functions.chunk_recursively,
            'k': 14
        },
        'semantic': {
            'function': my_rag_functions.chunk_semantic,
            'k': 68
        }
    }

    all_results = []

    for sample in tqdm(test_samples, desc='Processing Samples for Chunking experiment'):
        prompt = sample['Prompt']
        ground_truth_answer = sample['Answer']
        knowledge_urls = ast.literal_eval(sample['wiki_links'])

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text: continue

        sample_results = {
            'question': prompt,
            'standard_answer': ground_truth_answer
        }
        
        for name, config in chunking_configs.items():
            chunk_func = config['function']
            top_k = config['k']

            if name == 'semantic':
                text_chunks = chunk_func(full_context_text, client=dashscope_client)
            else:
                text_chunks = chunk_func(full_context_text)

            chunks_embeddings = my_rag_functions.vector_chunks_embedding(text_chunks, client=dashscope_client)
            retrieved_list = my_rag_functions.semantic_search(dashscope_client, prompt, text_chunks, chunks_embeddings, k=top_k)
            retrieved_content = ' '.join(retrieved_list)
            generate_answer = my_rag_functions.generate_response_deepseek(deepseek_client, retrieved_content, prompt)

            sample_results[f'generated_answer_{name}'] = generate_answer
            sample_results[f'is_correct_{name}'] = (generate_answer.strip().lower() == ground_truth_answer.strip().lower())

        all_results.append(sample_results)

    save_results(all_results, 'experiment_chunking')
    print('âœ… å®éªŒä¸€(åŠ¨æ€Top-K)å®Œæˆï¼')

def run_retrieval_experiment(test_sample):
    print('\n' + '='*50)
    print('å®éªŒäºŒï¼Œæ£€æµ‹ä¸åŒæ£€ç´¢(Retrieval)æ–¹æ³•')
    print('='*50)

    all_results = []

    for sample in tqdm(test_sample, desc='Processing Samples for Retrieval Experiment'):
        prompt = sample['Prompt']
        ground_truth_answer = sample['Answer']
        knowledge_urls = ast.literal_eval(sample['wiki_links'])

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text: continue

        text_chunks = my_rag_functions.chunk_by_fix_size(full_context_text)
        chunks_embeddings = my_rag_functions.vector_chunks_embedding(text_chunks, client=dashscope_client)

        sample_results = {
            'question': prompt,
            'standard_answer': ground_truth_answer
        }
        # æµ‹è¯•Dense Retrieval
        dense_results = my_rag_functions.semantic_search(dashscope_client, prompt, text_chunks, chunks_embeddings)
        retrieved_context_dense = ' '.join(dense_results)
        generated_answer_dense = my_rag_functions.generate_response_deepseek(deepseek_client, retrieved_context_dense, prompt)
        sample_results["generated_answer_dense"] = generated_answer_dense
        sample_results["is_correct_dense"] = (generated_answer_dense.strip().lower() == ground_truth_answer.strip().lower())

        # æµ‹è¯•Sparse Retrieval
        sparse_results = my_rag_functions.sparse_search_bm25(prompt, text_chunks)
        retrieved_context_sparse = ' '.join(sparse_results)
        generated_answer_sparse = my_rag_functions.generate_response_deepseek(deepseek_client, retrieved_context_sparse, prompt)
        sample_results["generated_answer_sparse"] = generated_answer_sparse
        sample_results["is_correct_sparse"] = (generated_answer_sparse.strip().lower() == ground_truth_answer.strip().lower())
            
        # æµ‹è¯•Hybrid Retrieval
        hybrid_results = my_rag_functions.hybrid_search_rrf(prompt, text_chunks, dense_results, sparse_results)
        retrieved_context_hybrid = ' '.join(hybrid_results)
        generated_answer_hybrid = my_rag_functions.generate_response_deepseek(deepseek_client, retrieved_context_hybrid, prompt)
        sample_results["generated_answer_hybrid"] = generated_answer_hybrid
        sample_results["is_correct_hybrid"] = (generated_answer_hybrid.strip().lower() == ground_truth_answer.strip().lower())

        # æµ‹è¯•Hybrid Retrieval + Rerank
        initial_candidates = my_rag_functions.hybrid_search_rrf(prompt, text_chunks, dense_results, sparse_results, k=20)
        reranked_results = my_rag_functions.rerank_results(prompt, initial_candidates, top_n=5)
        retrieved_context_reranked = ' '.join(reranked_results)
        generated_answer_reranked = my_rag_functions.generate_response_deepseek(deepseek_client, retrieved_context_reranked, prompt)
        sample_results['generated_answer_reranked'] = generated_answer_reranked
        sample_results['is_correct_reranked'] = (generated_answer_reranked.strip().lower() == ground_truth_answer.strip().lower())

        all_results.append(sample_results)

    save_results(all_results, 'experiment_retrieval')
    print('âœ… å®éªŒäºŒå®Œæˆï¼')

def run_best_rag_experiment(test_samples):
    print('\n' + '='*50)
    print('ğŸš€ å¼€å§‹æœ€ç»ˆæµ‹è¯•: è¿è¡Œæœ€å¼ºRAGç»„åˆ (ä¿®æ­£ç‰ˆ)')
    print('   - åˆ†å—: Semantic Chunking')
    print('   - æ£€ç´¢: Hybrid Search + Rerank')
    print('   - ç”Ÿæˆ: DeepSeek')
    print("="*50)
    
    all_results = []
    
    # --- [æ ¸å¿ƒä¿®æ­£] è°ƒæ•´Kå€¼ä»¥åŒ¹é…ä¸Šä¸‹æ–‡é¢„ç®— ---
    # åˆæ­¥æ£€ç´¢çš„Kå€¼åº”è¯¥é‡‡ç”¨ä¸ºsemantic chunkingè®¡ç®—å‡ºçš„å…¬å¹³Kå€¼
    INITIAL_K_FOR_SEMANTIC = 68
    # Rerankåæœ€ç»ˆå–‚ç»™LLMçš„æ•°é‡
    FINAL_K = 5

    for sample in tqdm(test_samples, desc="Processing Samples for Final Test"):
        prompt = sample['Prompt']
        ground_truth_answer = sample['Answer']
        try:
            knowledge_urls = ast.literal_eval(sample['wiki_links'])
        except (ValueError, SyntaxError):
            knowledge_urls = []

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text: continue

        # --- æ‰§è¡Œæœ€å¼ºRAGæµæ°´çº¿ ---
        
        # 1. åˆ†å— (Semantic Chunking)
        text_chunks = my_rag_functions.chunk_semantic(full_context_text, client=dashscope_client)
        if not text_chunks: continue

        # 2. å‘é‡åŒ–
        chunks_embeddings = my_rag_functions.vector_chunks_embedding(text_chunks, client=dashscope_client)
        if not chunks_embeddings: continue

        # 3. æ··åˆæ£€ç´¢ (è·å–Top-68ç²—æ’ç»“æœ)
        dense_results = my_rag_functions.semantic_search(dashscope_client, prompt, text_chunks, chunks_embeddings, k=INITIAL_K_FOR_SEMANTIC)
        sparse_results = my_rag_functions.sparse_search_bm25(prompt, text_chunks, k=INITIAL_K_FOR_SEMANTIC)
        retrieval_results = my_rag_functions.hybrid_search_rrf(prompt, text_chunks, dense_results, sparse_results, k=INITIAL_K_FOR_SEMANTIC)

        # 4. é‡æ’åº (ä»68ä¸ªå€™é€‰ä¸­ï¼Œè·å–Top-5ç²¾æ’ç»“æœ)
        # reranked_results = my_rag_functions.rerank_results(prompt, initial_candidates, top_n=FINAL_K)
        
        # 5. ç”Ÿæˆç­”æ¡ˆ
        retrieval_context = ' '.join(retrieval_results)
        generated_answer = my_rag_functions.generate_response(dashscope_client, retrieval_context, prompt)
        
        is_correct = (generated_answer.strip().lower() == ground_truth_answer.strip().lower())

        # è®°å½•ç»“æœ
        all_results.append({
            "question": prompt,
            "standard_answer": ground_truth_answer,
            "generated_answer": generated_answer,
            "is_correct": is_correct
        })

    save_results(all_results, "experiment_best_rag_combination_fixed")
    
    # è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆå‡†ç¡®ç‡
    correct_count = sum(1 for r in all_results if r['is_correct'])
    total_count = len(all_results)
    if total_count > 0:
        accuracy = correct_count / total_count
        print(f'\nğŸ† æœ€å¼ºRAGç»„åˆåœ¨ {total_count} ä¸ªæ ·æœ¬ä¸Šæœ€ç»ˆæ­£ç¡®ç‡ä¸º: {accuracy:.2%}')
    
    print("âœ… æœ€ç»ˆæµ‹è¯•å®Œæˆï¼")

def run_final_experiment(test_samples):
    print('\n' + '='*50)
    print('ğŸš€ å¼€å§‹æœ€ç»ˆå®éªŒï¼šå…¬å¹³æ¡ä»¶ä¸‹å¯¹æ¯”åˆ†å—ç­–ç•¥')
    print('   - æ£€ç´¢ç­–ç•¥: Hybrid Retrieval (å›ºå®š)')
    print('   - ç”Ÿæˆæ¨¡å‹: Kimi-k2 (å›ºå®š)')
    print("="*50)

    chunking_configs = {
        'fix_size_150': {
            'function': lambda text: my_rag_functions.chunk_by_fix_size(text, chunk_size=150),
            'k': 65
        },
        'recursive_150':{
            'function': lambda text: my_rag_functions.chunk_recursively(text, chunk_size=150),
            'k': 65
        },
        'semantic':{
            'function': lambda text: my_rag_functions.chunk_semantic(text, client=dashscope_client),
            'k': 65
        }
    }

    all_results = []

    for sample in tqdm(test_samples, desc="Processing Samples for Final Experiment"):
        prompt = sample['Prompt']
        ground_truth_answer = sample['Answer']
        try:
            knowledge_urls = ast.literal_eval(sample['wiki_links'])
        except (ValueError, SyntaxError):
            knowledge_urls = []

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text: continue

        sample_results = {"question": prompt, "standard_answer": ground_truth_answer}

        for name, config in chunking_configs.items():
            chunk_func = config["function"]
            top_k = config["k"]

            # 1. åˆ†å—
            text_chunks = chunk_func(full_context_text)
            if not text_chunks: continue

            # 2. å‘é‡åŒ–
            chunks_embeddings = my_rag_functions.vector_chunks_embedding(text_chunks, client=dashscope_client)
            if not chunks_embeddings or len(text_chunks) != len(chunks_embeddings): continue
            
            # 3. æ··åˆæ£€ç´¢ (å›ºå®šä½¿ç”¨Hybrid)
            dense_results = my_rag_functions.semantic_search(dashscope_client, prompt, text_chunks, chunks_embeddings, k=top_k)
            sparse_results = my_rag_functions.sparse_search_bm25(prompt, text_chunks, k=top_k)
            retrieved_results = my_rag_functions.hybrid_search_rrf(prompt, text_chunks, dense_results, sparse_results, k=top_k)
            
            # 4. ç”Ÿæˆç­”æ¡ˆ (å›ºå®šä½¿ç”¨Kimik2)
            retrieved_context = ' '.join(retrieved_results)
            generated_answer = my_rag_functions.generate_response_kimik2(kimik2_client, retrieved_context, prompt)
            
            # 5. è®°å½•ç»“æœ
            sample_results[f"generated_answer_{name}"] = generated_answer
            sample_results[f"is_correct_{name}"] = (generated_answer.strip().lower() == ground_truth_answer.strip().lower())
        
        all_results.append(sample_results)

    save_results(all_results, "experiment_final_chunking_vs_hybrid")
    print("âœ… æœ€ç»ˆå®éªŒå®Œæˆï¼")

def experiment_rag0728(test_samples):
    """
    RAGç»ˆæå¯¹å†³å®éªŒ (2025/07/28)
    ç›®æ ‡ï¼šåœ¨æœ€å¼ºçš„â€œHybrid Search + Rerankâ€åç«¯åŠ æŒä¸‹ï¼Œæ¨ªå‘å¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„æœ€ç»ˆæ€§èƒ½ã€‚
    """
    print('\n' + '='*80)
    print('ğŸš€ğŸ§ª Hybrid + Rerank vs. ä¸åŒåˆ†å—ç­–ç•¥ ğŸ§ªğŸš€')
    print('='*80)

    # --- æ ¸å¿ƒå‚æ•°å®šä¹‰ ---
    # K_RETRIEVAL: åˆæ­¥æ£€ç´¢æå›çš„å€™é€‰æ–‡æ¡£æ•°é‡ï¼Œç”¨äºRerankçš„è¾“å…¥æ± 
    K_RETRIEVAL = 50  # ä¸€ä¸ªåœ¨æ€§èƒ½å’Œç®—åŠ›ä¹‹é—´è¾ƒä¸ºå‡è¡¡çš„é€‰æ‹©
    # N_RERANK: ç»è¿‡Rerankåï¼Œæœ€ç»ˆå–‚ç»™LLMçš„æœ€ç›¸å…³æ–‡æ¡£æ•°é‡
    N_RERANK = 5      # é»„é‡‘ä¸Šä¸‹æ–‡çª—å£å¤§å°

    print(f"[*] å®éªŒé…ç½®: åˆæ­¥æ£€ç´¢ Top-K (k_retrieval) = {K_RETRIEVAL}, æœ€ç»ˆç²¾æ’ Top-N (n_rerank) = {N_RERANK}")
    
    chunking_configs = {
        'semantic': {
            'function': lambda text: my_rag_functions.chunk_semantic(text, client=dashscope_client),
        },
        'fix_size_250': {
            'function': lambda text: my_rag_functions.chunk_by_fix_size(text, chunk_size=250),
        },
        'recursive_250': {
            'function': lambda text: my_rag_functions.chunk_recursively(text, chunk_size=250, chunk_overlap=30),
        }
    }

    all_results = []
    # ç”¨äºåˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªç­–ç•¥çš„å‡†ç¡®ç‡
    accuracy_trackers = {name: {'correct': 0, 'total': 0} for name in chunking_configs.keys()}

    for sample in tqdm(test_samples, desc="[ç»ˆæå¯¹å†³] å¤„ç†æ ·æœ¬ä¸­..."):
        prompt = sample['Prompt']
        ground_truth_answer = sample['Answer']
        try:
            knowledge_urls = ast.literal_eval(sample['wiki_links'])
        except (ValueError, SyntaxError):
            knowledge_urls = []

        full_context_text = get_content_from_urls(knowledge_urls)
        if not full_context_text:
            continue

        sample_results = {"question": prompt, "standard_answer": ground_truth_answer}

        for name, config in chunking_configs.items():
            print(f"\n--- Running for: {name} ---")
            
            # 1. åˆ†å— (Chunking)
            chunk_func = config["function"]
            text_chunks = chunk_func(full_context_text)
            if not text_chunks:
                print(f"è­¦å‘Š: {name} åˆ†å—ç­–ç•¥æœªäº§ç”Ÿä»»ä½•chunksï¼Œè·³è¿‡æ­¤æ ·æœ¬ã€‚")
                continue
            
            # 2. å‘é‡åŒ– (Embedding)
            chunks_embeddings = my_rag_functions.vector_chunks_embedding(text_chunks, client=dashscope_client)
            if not chunks_embeddings or len(text_chunks) != len(chunks_embeddings):
                print(f"è­¦å‘Š: å‘é‡åŒ–å¤±è´¥æˆ–æ•°é‡ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤æ ·æœ¬ã€‚")
                continue

            # 3. åˆæ­¥æ£€ç´¢ (Retrieval - Hybrid Search)
            # ç›®æ ‡ï¼šå¹¿æ’’ç½‘ï¼Œæå› K_RETRIEVAL ä¸ªå€™é€‰å—
            print(f"   -> (1/3) æ··åˆæ£€ç´¢ (Hybrid Search) k={K_RETRIEVAL}...")
            dense_results = my_rag_functions.semantic_search(dashscope_client, prompt, text_chunks, chunks_embeddings, k=K_RETRIEVAL)
            sparse_results = my_rag_functions.sparse_search_bm25(prompt, text_chunks, k=K_RETRIEVAL)
            initial_candidates = my_rag_functions.hybrid_search_rrf(prompt, text_chunks, dense_results, sparse_results, k=K_RETRIEVAL)
            
            # 4. ç²¾ç¡®é‡æ’ (Rerank)
            # ç›®æ ‡ï¼šç²¾æŒ‘ç»†é€‰ï¼Œä»å€™é€‰æ± ä¸­é€‰å‡ºæœ€ç›¸å…³çš„ N_RERANK ä¸ª
            print(f"   -> (2/3) ç²¾ç¡®é‡æ’ (Rerank) n={N_RERANK}...")
            final_chunks = my_rag_functions.rerank_results(prompt, initial_candidates, top_n=N_RERANK)
            
            # 5. ç­”æ¡ˆç”Ÿæˆ (Generation)
            # ç›®æ ‡ï¼šåŸºäºé»„é‡‘ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            print(f"   -> (3/3) ç”Ÿæˆç­”æ¡ˆ (Generation)...")
            retrieved_context = ' '.join(final_chunks)
            generated_answer = my_rag_functions.generate_response_kimik2(kimik2_client, retrieved_context, prompt)
            
            # 6. è®°å½•ç»“æœ
            is_correct = (str(generated_answer).strip().lower() == str(ground_truth_answer).strip().lower())
            sample_results[f"generated_answer_{name}"] = generated_answer
            sample_results[f"is_correct_{name}"] = is_correct
            
            accuracy_trackers[name]['correct'] += 1 if is_correct else 0
            accuracy_trackers[name]['total'] += 1
            
        all_results.append(sample_results)

    # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
    save_results(all_results, "experiment_rag0728_ultimate_showdown")
    
    # æ‰“å°æœ€ç»ˆçš„å‡†ç¡®ç‡å¯¹æ¯”æŠ¥å‘Š
    print('\n' + '='*80)
    print("ğŸ“Š ç»ˆæå¯¹å†³å®éªŒç»“æœæŠ¥å‘Š ğŸ“Š")
    print('='*80)
    for name, stats in accuracy_trackers.items():
        if stats['total'] > 0:
            accuracy = (stats['correct'] / stats['total']) * 100
            print(f"ğŸ† ç­–ç•¥: {name:<20} | å‡†ç¡®ç‡: {accuracy:.2f}% ({stats['correct']}/{stats['total']})")
        else:
            print(f"âš ï¸ ç­–ç•¥: {name:<20} | æœªè¿è¡Œæˆ–æ— æœ‰æ•ˆæ ·æœ¬ã€‚")
    print('='*80)
    print("âœ… æ‰€æœ‰å®éªŒå·²å®Œæˆï¼")



if __name__ == '__main__':
    print("æ­£åœ¨åŠ è½½FRAMESæ•°æ®é›†...")
    frames_dataset = load_dataset('google/frames-benchmark')
    test_set = frames_dataset['test']

    # å»ºè®®æµ‹è¯•æ ·æœ¬æ•°é‡ä¸è¦å¤ªå¤šï¼Œå› ä¸ºRerankä¼šæ¯”è¾ƒè€—æ—¶
    shuffled_test_set = test_set.shuffle(seed=42)
    test_samples = list(shuffled_test_set.select(range(20))) 
    print(f"å·²éšæœºæŒ‘é€‰ {len(test_samples)} ä¸ªæ ·æœ¬è¿›è¡Œæœ€ç»ˆå¯¹å†³æµ‹è¯•ã€‚")

    # --- è°ƒç”¨ä½ çš„æ–°å®éªŒå‡½æ•° ---
    experiment_rag0728(test_samples)

    print("\nğŸ‰ æ‰€æœ‰å®éªŒå·²å®Œæˆï¼")
